{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Comprehensive Introduction of Medusa\n",
    "Sept, 2023 | 45 min | Yuhong Li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # define GPU id, remove if you want to use all GPUs available\n",
    "import torch\n",
    "from contextlib import contextmanager\n",
    "import numpy as np\n",
    "from medusa.model.medusa_model import MedusaModel\n",
    "from medusa.model.kv_cache import *\n",
    "from medusa.model.utils import *\n",
    "from medusa.model.medusa_choices import *\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How LLM inference?\n",
    "\n",
    "In this section, we will illustrate LLM inference with KV cache mechanism.\n",
    "\n",
    "The Key-Value (KV) cache mechanism serves as a rapid-access storage during inference. By retaining the model's intermediate KV features from previous inferences in this cache, computation time can be substantially reduced.\n",
    "\n",
    "\n",
    "Consider an example where the incoming input token has a shape of `[1, 1]`, denoting a batch_size of 1 and a sequence_length of 1. If the model has prior intermediate KV features stored in the cache, the inference will proceed as follows:\n",
    "\n",
    "\n",
    "1. **Embedding Mapping**: The model transforms the input token into an embedding vector, resulting in a shape of `[1, 1, hidden_size]`.\n",
    "\n",
    "2. **Forward through FFN**: This embedded vector is then passed through the Feed Forward Network (FFN) module, yielding an output with the same shape: `[1, 1, hidden_size]`.\n",
    "\n",
    "3. **Attention Mechanism**: \n",
    "\n",
    "    * The query vector for the current token is reshaped and transformed from `[1, 1, hidden_size]` to `[1, 1, num_heads, head_dim]` to `[1, num_heads, 1, head_dim]`.\n",
    "    * The cached intermediate KV vectors from previous inferences have the shape `[1, num_heads, prev_kv_seq_len, head_dim]`.\n",
    "    * Current KV vectors are concatenated with these cached KV vectors, resulting in a new shape of `[1, num_heads, prev_kv_seq_len + 1, head_dim]`.\n",
    "    * Hence, the QK matrix's shape is `[1, num_heads, 1, head_dim] x [1, num_heads, head_dim, prev_kv_seq_len + 1]` -> `[1, num_heads, 1, prev_kv_seq_len + 1]`.\n",
    "    * The (QK)V matrix's shape is `[1, num_heads, 1, prev_kv_seq_len + 1] x [1, num_heads, prev_kv_seq_len + 1, head_dim]` -> `[1, num_heads, 1, head_dim]`.\n",
    "    * The output is transformed and reshaped to `[1, 1, hidden_size]`.\n",
    "\n",
    "Without the KV cache mechanism, the inference will proceed the **whole sequence** each time, resulting in significantly larger QK matrix and slower FFN computation.\n",
    "\n",
    "If you want to know more about KV cache, please refer to the awesome [blog](https://www.dipkumar.dev/becoming-the-unbeatable/posts/gpt-kvcache/).\n",
    "\n",
    "Let's try an example with KV cache inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CACHE_DIR = '/data/taehyeon/'\n",
    "CACHE_DIR = '/mnt/data1/taehyeon/'\n",
    "model_name = 'FasterDecoding/medusa-vicuna-7b-v1.3'\n",
    "model = MedusaModel.from_pretrained(\n",
    "    model_name,\n",
    "    # medusa_num_heads = 4,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\",\n",
    "    cache_dir=CACHE_DIR\n",
    ")\n",
    "tokenizer = model.get_tokenizer()\n",
    "\n",
    "medusa_choices = mc_sim_7b_63\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the kv_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_key_values, past_key_values_data, current_length_data = initialize_past_key_values(model.base_model)\n",
    "model.past_key_values = past_key_values\n",
    "model.past_key_values_data = past_key_values_data\n",
    "model.current_length_data = current_length_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the prompt\n",
    "\n",
    "The prompt is as follow:\n",
    "\n",
    "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: Hi, could you share a tale about a charming llama that grows Medusa-like hair and starts its own coffee shop? ASSISTANT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.current_length_data.zero_() # this is for rerun\n",
    "prompt = \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: Hi, could you share a tale about a charming llama that grows Medusa-like hair and starts its own coffee shop? ASSISTANT:\"\n",
    "print(prompt)\n",
    "input_ids = tokenizer([prompt]).input_ids\n",
    "input_len = len(input_ids[0])\n",
    "print('Input token length:', len(input_ids[0]))\n",
    "print('Init KV cache shape for attention modules:', model.past_key_values[0][0].shape, model.past_key_values[0][1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First inference for all the prompt tokens\n",
    "\n",
    "You can see the input is the prompt which contains 66 tokens. The output is the logits (`batchsize x seq_len x class`) of the predicted next token for each token in the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.base_model.model(torch.as_tensor(input_ids).cuda(), past_key_values=model.past_key_values,)\n",
    "orig  = model.base_model.lm_head(output[0])\n",
    "print(len((input_ids[0])), orig.shape)\n",
    "orig1 = output[0].clone()\n",
    "new = torch.cat([output[0], orig1], dim=-1)\n",
    "a= torch.chunk(new, 2, dim=-1)\n",
    "torch.stack(a, dim=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    model.current_length_data.zero_() # this is for rerun\n",
    "    output = model.base_model(torch.as_tensor(input_ids).cuda(), past_key_values=model.past_key_values,)\n",
    "    print('output shape:', output.logits.shape)\n",
    "    pred = output.logits.argmax(-1)\n",
    "    input_ids[0] = input_ids[0] + pred[0, -1:].tolist()\n",
    "    print('KV cache shape for attention modules after first inference:', model.past_key_values[0][0].shape, model.past_key_values[0][1].shape)\n",
    "    print('First prediction:', tokenizer.batch_decode(pred[..., -1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second inference using the kv_cache\n",
    "You can see we utilize the KV cache to streamline our computations. Instead of feeding the entire prompt with new tokens to the model again, we only input the token that was predicted in the previous inference step. Notice this time the output `seq_len = 1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    output = model.base_model(pred[..., -1:], past_key_values=model.past_key_values, use_cache=True) # note we only need to put last token in the input_ids\n",
    "    print('output shape:', output.logits.shape)\n",
    "    pred = output.logits.argmax(-1)\n",
    "    input_ids[0] = input_ids[0] + pred[0, -1:].tolist()\n",
    "    print('KV cache shape for attention modules after second inference:', model.past_key_values[0][0].shape, model.past_key_values[0][1].shape)\n",
    "    print('Second prediction:', tokenizer.batch_decode(pred[..., -1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    for _ in range(10):\n",
    "        output = model.base_model(pred[..., -1:], past_key_values=model.past_key_values, use_cache=True)\n",
    "        pred = output.logits.argmax(-1)\n",
    "        # pred_topk = output.logits.topk(10, dim = -1).indices[0]\n",
    "        input_ids[0] = input_ids[0] + pred[0, -1:].tolist()\n",
    "        print(tokenizer.batch_decode(pred[..., -1:]))\n",
    "        # print(tokenizer.batch_decode(pred_topk), 'topk:', pred_topk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(input_ids[0][-12:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reaching Equilibrium in Model Output\n",
    "\n",
    "A central concept, especially when we discuss Medusa verification, is the equilibrium state of a model's output.\n",
    "\n",
    "Think about a sequence produced by the model. This sequence is composed of two parts:\n",
    "\n",
    "* **Prompt**: This is the initial input given to the model.\n",
    "\n",
    "* **Predictions**: These are the tokens that the model generates in response to the prompt.\n",
    "\n",
    "Merging these segments gives us the entire sequence, `concat([prompt, preds])`. For clarity, let's denote the `prompt` length as `n` and the `preds` length as `k`.\n",
    "\n",
    "One notable characteristic is that if this combined sequence is re-inputted into the model, it reproduces the same predictions. It implies that the pred section of the combined sequence has attained an equilibrium due to the model's generation capacity.\n",
    "\n",
    "For better understanding, here's a visual representation:\n",
    "\n",
    "\n",
    "<table border=\"1\">\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>input (<code>prompt</code>)</th>\n",
    "            <td><code>...</code></td>\n",
    "            <td>'shop'</td>\n",
    "            <td>'?'</td>\n",
    "            <td>'A'</td>\n",
    "            <td>'SS'</td>\n",
    "            <td>'IST'</td>\n",
    "            <td>'ANT'</td>\n",
    "            <td>':'</td>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <th>model output</th>\n",
    "            <td><code>...</code></td>\n",
    "            <td>'?'<br>&#9989</td>\n",
    "            <td>'\\n'<br>&#x274C</td>\n",
    "            <td>'story'<br>&#x274C</td>\n",
    "            <td>'S'<br>&#x274C</td>\n",
    "            <td>'\\n'<br>&#x274C</td>\n",
    "            <td>'I'<br>&#x274C</td>\n",
    "            <td><code>...</code></td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "Cont.\n",
    "\n",
    "<table border=\"1\">\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>input (<code>preds</code>)</th>\n",
    "            <td><code>...</code></td>\n",
    "            <td>'Once'</td>\n",
    "            <td>'upon'</td>\n",
    "            <td>'a'</td>\n",
    "            <td>'time'</td>\n",
    "            <td>','</td>\n",
    "            <td>'in'</td>\n",
    "            <td>'a'</td>\n",
    "            <td>'small'</td>\n",
    "            <td>'village'</td>\n",
    "            <td>'nest'</td>\n",
    "            <td>'led'</td>\n",
    "            <td>'in'</td>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <th>model output</th>\n",
    "            <td> 'Once' <br>&#9989</td>\n",
    "            <td>'upon' <br>&#9989</td>\n",
    "            <td>'a' <br>&#9989</td>\n",
    "            <td>'time' <br>&#9989</td>\n",
    "            <td>',' <br>&#9989</td>\n",
    "            <td>'in' <br>&#9989</td>\n",
    "            <td>'a' <br>&#9989</td>\n",
    "            <td>'small' <br>&#9989</td>\n",
    "            <td>'village' <br>&#9989</td>\n",
    "            <td>'nest' <br>&#9989</td>\n",
    "            <td>'led' <br>&#9989</td>\n",
    "            <td>'in' <br>&#9989</td>\n",
    "            <td>'the' </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "\n",
    "In this example, the top row displays the input `concat([prompt, preds])`. The second row represents the model's output tokens - essentially shifted input tokens, with the end-result being the upcoming token. While the prompt deviates since it isn't a direct result of the model's generation, the output tokens spanning from `[n - 1: n - 1 + k]` align perfectly with the input tokens from `[n: n + k]`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    output = model.base_model(torch.as_tensor(input_ids).cuda()).logits\n",
    "    pred = output.argmax(-1)\n",
    "    print(tokenizer.batch_decode(input_ids[0][-20:]))\n",
    "    print(tokenizer.batch_decode(pred[0,-20:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disturbing the Equilibrium State with a Minor Change\n",
    "\n",
    "To emphasize the sensitivity of the equilibrium state, let's consider another example.\n",
    "\n",
    "\n",
    "From our prior illustration, the sequence \"**Once upon a time, in a small village nestled in**\" had reached an equilibrium state, where feeding this sequence back into the model yielded the same predictions.\n",
    "\n",
    "But what happens when we introduce a minor change to this equilibrium?\n",
    "\n",
    "For the sake of this demonstration, we'll swap the word '**village**' with '**mountain**'. Both of these terms, from grammatical and semantic perspectives, fit well within the tale. Yet, as we'll see, the model's subsequent outputs following the word 'mountain' deviate drastically from our original sequence.\n",
    "\n",
    "Here's the visual representation:\n",
    "\n",
    "\n",
    "<table border=\"1\">\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>input</th>\n",
    "            <td><code>...</code></td>\n",
    "            <td>':'</td>\n",
    "            <td>'Once'</td>\n",
    "            <td>'upon'</td>\n",
    "            <td>'a'</td>\n",
    "            <td>'time'</td>\n",
    "            <td>','</td>\n",
    "            <td>'in'</td>\n",
    "            <td>'a'</td>\n",
    "            <td>'small'</td>\n",
    "            <td>'mountain'&#128221</td>\n",
    "            <td>'nest'</td>\n",
    "            <td>'led'</td>\n",
    "            <td>'in'</td>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <th>model output</th>\n",
    "            <td><code>...</code></td>\n",
    "            <td>'Once'<br>&#9989</td>\n",
    "            <td>'upon'<br>&#9989</td>\n",
    "            <td>'a'<br>&#9989</td>\n",
    "            <td>'time'<br>&#9989</td>\n",
    "            <td>','<br>&#9989</td>\n",
    "            <td>'in'<br>&#9989</td>\n",
    "            <td>'a'<br>&#9989</td>\n",
    "            <td>'small'<br>&#9989</td>\n",
    "            <td>'village'<br>&#9989</td>\n",
    "            <td>'village'<br>&#x274C</td>\n",
    "            <td>'led'<br>&#9989</td>\n",
    "            <td>'between'<br>&#x274C</td>\n",
    "            <td>'the'<br>&#10067</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    input_ids_mod = deepcopy(input_ids)\n",
    "    input_ids_mod[0][-4] = 14378 # replace 'village' with 'mountain'\n",
    "    output = model.base_model(torch.as_tensor(input_ids_mod).cuda()).logits\n",
    "    pred = output.argmax(-1)\n",
    "    print(tokenizer.batch_decode(input_ids_mod[0][-20:]))\n",
    "    print(tokenizer.batch_decode(pred[0,-20:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, rethinking the problem in another way. You are given the previous `prompt`, and a fortune teller gazed deep into her crystal ball and whispers, \"Ah, the ancient spirit of Vicuna spoke to me. It said: **Once upon a time, in a small moutain nestled in...**\"\n",
    "\n",
    "Doubt creeps into your mind. Is her prediction genuine? With a mix of skepticism and curiosity, you challenged her, \"Let us perform one inference to validate your foresight.\"\n",
    "\n",
    "You then input the combination of her revelation and the original prompt into the model. Astoundingly, the model's output mirrors her prediction to the letter... until a deviation at the word '**village**'.\n",
    "\n",
    "OK, you might realize the fortune teller is what we called '**speculative decoding**', a small model or Medusa heads that can draft next several tokens for you.\n",
    "\n",
    "For this particular round, let's determine the number of accurate tokens the fortune teller has provided.\n",
    "\n",
    "You initiate a single inference and notice that for the most recent prompt token, '**:**', the model's output is '**Once**'. This is the model's **own** autoregressive generation and it is in the equilibrium state. Even without her, you still know the model will output '**Once**' for the input '**:**'!\n",
    "\n",
    "That means she correctly predicted '**upon a time, in a small**', didn't she?\n",
    "\n",
    "But wait! Something seems off. You verified '**small**' is correct. And what about the output for the input token '**small**'? Since the previous predictions all stay equilibrium, the model's output for 'small', '**village**' should be in the equilibrium state.\n",
    "\n",
    "So eventhough she didn't predict '**village**' correctly, through one inference, you can still get the correct autoregressive result for the input '**small**'!\n",
    "\n",
    "Consider this: in the extreme scenario where the fortune teller is completely off the mark with her predictions, the model would still autonomously generate 1 correct token in response to the prompt.\n",
    "\n",
    "The inference verification inherently ensures a lower bound **1** for correct prediction. In other words, the verification guarantees at least **1** correct token in its predictions.\n",
    "\n",
    "Hence, if everything goes off, the verification is just a regular autoregressive inference. As a result, every step necessitates only one inference for both verification and autoregressive processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are Medusa Heads?\n",
    "\n",
    "So what exactly are Medusa heads? At a glance, they might be reminiscent of the language model head in the original architecture, particularly the last layer of a causal Transformer model. However, there's a distinguishing factor. Instead of predicting solely the immediate next token, Medusa heads are designed to predict multiple upcoming tokens. This intriguing feature has its roots in the [Blockwise Parallel Decoding approach](https://arxiv.org/abs/1811.03115). To realize this, each Medusa head is structured as a feed-forward network, and to enhance its efficiency, it is further complemented with a residual connection.\n",
    "\n",
    "We print the Medusa heads' structure below that contains 4 identical module. Each module starts with a residual block, consisting of a linear layer followed by a SiLU activation function, and concludes with another linear layer which outputs the classification results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the structure of medusa heads\n",
    "print(model.medusa_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial inference with Medusa heads\n",
    "\n",
    "Here, we perform the first inference with Medusa heads. We feed the prompt into the model and get the model's head's prediction and the Medusa heads' prediction:\n",
    "\n",
    "<table border=\"1\">\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th> </th>\n",
    "            <th>Model's head</th>\n",
    "            <th>Medusa head 1</th>\n",
    "            <th>Medusa head 2</th>\n",
    "            <th>Medusa head 3</th>\n",
    "            <th>Medusa head 4</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <th> Prediction </th>\n",
    "            <td>'Once'<br>&#9989</td>\n",
    "            <td>'upon'<br>&#10067</td>\n",
    "            <td>'ly'<br>&#10067</td>\n",
    "            <td>'time'<br>&#10067</td>\n",
    "            <td>','<br>&#10067</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "Since only the model's head's prediction is in the equilibrium state and the Medusa heads' are unknown, we need to perform second inference to verify the Medusa heads' prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    input_ids = tokenizer([prompt]).input_ids\n",
    "    input_len = len(input_ids[0])\n",
    "    input_ids = torch.as_tensor(input_ids).cuda()\n",
    "    model.current_length_data.zero_() # this is for rerun\n",
    "    medusa_logits, outputs, logits = model(input_ids, output_orig = True, past_key_values=model.past_key_values)\n",
    "print('Medusa logits shape:', medusa_logits.shape, 'logits shape:', logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medusa_pred = torch.argmax(medusa_logits[..., -1, :], dim = -1)\n",
    "pred = torch.argmax(logits[..., -1, :], dim = -1)\n",
    "print('Base model prediction:', tokenizer.batch_decode(pred))\n",
    "print('Medusa prediction:', tokenizer.batch_decode(medusa_pred))\n",
    "preds = torch.cat([pred, medusa_pred[:, 0 ]], dim = -1)\n",
    "print('Combined prediction:', tokenizer.batch_decode(preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second inference with Medusa heads\n",
    "\n",
    "We feed the 5 tokens into the model together with the past KV cache. \n",
    "\n",
    "<table border=\"1\">\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>Input</th>\n",
    "            <td>'Once'</td>\n",
    "            <td>'upon'</td>\n",
    "            <td>'ly'</td>\n",
    "            <td>'time'</td>\n",
    "            <td>','</td>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <th>Model output</th>\n",
    "            <td>'upon'<br>&#9989</td>\n",
    "            <td>'a'<br>&#9989</td>\n",
    "            <td>'a'<br>&#x274C</td>\n",
    "            <td>','<br>&#x274C</td>\n",
    "            <td>'in'<br>&#x274C</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "Though the Medusa heads only predict '**upon**' correctly, recall from the previous section of the model's equilibrium property, we still can get the correct autoregressive result '**a**' for the input '**upon**'!\n",
    "\n",
    "Let's calculate how many tokens in the equilibrium state we can get from the initial and second inference.\n",
    "\n",
    "* Initial inference: Since we didn't verify the Medusa heads' prediction, we only get 1 token in the equilibrium state, which is the model's head's prediction '**Once**'.\n",
    "* Second inference: We verify the Medusa heads' prediction '**upon**', and we know the next token '**a**' is in the equilibrium state. So we get 2 tokens in the equilibrium state.\n",
    "\n",
    "In total, we get 3 tokens in the equilibrium state. Current acceleration ratio is 3/2 = **1.5x**.\n",
    "\n",
    "Then, we need to prepare the input for the third inference. We combine the '**upon**''s output with its Medusa heads' prediction as the next input.\n",
    "\n",
    "<table border=\"1\">\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th> </th>\n",
    "            <th>Model's head</th>\n",
    "            <th>Medusa head 1</th>\n",
    "            <th>Medusa head 2</th>\n",
    "            <th>Medusa head 3</th>\n",
    "            <th>Medusa head 4</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <th> Prediction </th>\n",
    "            <td>'a'<br>&#9989</td>\n",
    "            <td>'time'<br>&#10067</td>\n",
    "            <td>','<br>&#10067</td>\n",
    "            <td>'there'<br>&#10067</td>\n",
    "            <td>'a'<br>&#10067</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "Another tricky thing we have to deal with is that the past KV cache already contains medusa heads' KV cache with total length 66 (`prompt`) + 5 (`preds`) = 71, so we have a KV management system to reset the KV cache to correct length (66 + 2 = 68)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "  medusa_logits, outputs, logits = model(preds.cuda().unsqueeze(0), output_orig = True, past_key_values = model.past_key_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medusa_pred = torch.argmax(medusa_logits[..., -5:, :], dim = -1)\n",
    "pred = torch.argmax(logits[..., :, :], dim = -1)\n",
    "print('Base model prediction:', tokenizer.batch_decode(pred[0]))\n",
    "print('truncated input tokens:', preds[1:].tolist())\n",
    "print('Output tokens:', pred[0, :].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_mask = (\n",
    "            preds[1:] == pred[0, :-1]\n",
    "        ).int()\n",
    "accept_length = torch.cumprod(posterior_mask, dim = -1).sum().item()\n",
    "cur_length = accept_length + input_len + 1\n",
    "print('Posterior mask:', posterior_mask.tolist())\n",
    "print('Accept length:', accept_length)\n",
    "print('Current KV cache length for attention modules:', model.current_length_data[0].item())\n",
    "print('Start length:', input_len, ',current length:', cur_length)\n",
    "# update kv cache\n",
    "model.current_length_data.fill_(cur_length)\n",
    "# create new input\n",
    "preds = torch.cat([pred[:, accept_length], medusa_pred[:,0,accept_length]], dim = -1)\n",
    "print('Combined prediction:', tokenizer.batch_decode(preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third inference with Medusa heads\n",
    "\n",
    "Let's practice another inference and reinforce our understanding!\n",
    "\n",
    "Outputs of the third inference:\n",
    "\n",
    "<table border=\"1\">\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>Input</th>\n",
    "            <td>'a'</td>\n",
    "            <td>'time'</td>\n",
    "            <td>','</td>\n",
    "            <td>'there'</td>\n",
    "            <td>'a'</td>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <th>Model output</th>\n",
    "            <td>'time'<br>&#9989</td>\n",
    "            <td>','<br>&#9989</td>\n",
    "            <td>'in'<br>&#9989</td>\n",
    "            <td>'was'<br>&#x274C</td>\n",
    "            <td>'ll'<br>&#x274C</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "This time, the model correctly predict '**time**' and '**,**', and '**there**' and '**a**' are incorrect. However, we still get the correct autoregressive result '**in**' for the input '**,**'!\n",
    "\n",
    "So how many tokens the original model outputs, or in the equilibrium state, can we get from the 3 inference?\n",
    "\n",
    "* Initial inference & second inference: see what we discussed above, we get 3 tokens in the equilibrium state.\n",
    "* Third inference: we verify the Medusa heads' prediction '**time**' and '**,**', and the next token '**in**' is in the equilibrium state. So we get 3 tokens in the equilibrium state.\n",
    "\n",
    "In total, we get 6 tokens in the equilibrium state. Acceleration ratio for the first 3 inferences is 6/3 = **2x**.\n",
    "\n",
    "And then, we combine the '**upon**''s output with its Medusa heads' prediction as the next input.\n",
    "\n",
    "<table border=\"1\">\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th> </th>\n",
    "            <th>Model's head</th>\n",
    "            <th>Medusa head 1</th>\n",
    "            <th>Medusa head 2</th>\n",
    "            <th>Medusa head 3</th>\n",
    "            <th>Medusa head 4</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <th> Prediction </th>\n",
    "            <td>'in'<br>&#9989</td>\n",
    "            <td>'a'<br>&#10067</td>\n",
    "            <td>'a'<br>&#10067</td>\n",
    "            <td>'far'<br>&#10067</td>\n",
    "            <td>'off'<br>&#10067</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    medusa_logits, outputs, logits = model(preds.cuda().unsqueeze(0), output_orig = True, past_key_values = model.past_key_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medusa_pred = torch.argmax(medusa_logits[..., -5:, :], dim = -1)\n",
    "pred = torch.argmax(logits[..., :, :], dim = -1)\n",
    "print('Base model prediction:', tokenizer.batch_decode(pred[0]))\n",
    "print('truncated input tokens:', preds[1:].tolist())\n",
    "print('Output tokens:', pred[0, :].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_mask = (\n",
    "            preds[1:] == pred[0, :-1]\n",
    "        ).int()\n",
    "accept_length = torch.cumprod(posterior_mask, dim = -1).sum().item()\n",
    "cur_length = accept_length + cur_length + 1\n",
    "print('Posterior mask:', posterior_mask.tolist())\n",
    "print('Accept length:', accept_length)\n",
    "print('Current KV cache length for attention modules:', model.current_length_data[0].item())\n",
    "print('Start length:', input_len, 'current length:', cur_length)\n",
    "# update kv cache\n",
    "model.current_length_data.fill_(cur_length)\n",
    "# create new input\n",
    "preds = torch.cat([pred[:, accept_length], medusa_pred[:,0,accept_length]], dim = -1)\n",
    "print('Combined prediction:', tokenizer.batch_decode(preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's put it all together!\n",
    "\n",
    "We now have a better understanding of the Medusa heads' inference process. Let's put the entire process together and see the compression ratio we can get for generating the whole conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_count = 0\n",
    "accept_lengths = []\n",
    "with torch.inference_mode():\n",
    "    input_ids = tokenizer([prompt]).input_ids\n",
    "    input_len = len(input_ids[0])\n",
    "    input_ids = torch.as_tensor(input_ids).cuda()\n",
    "    model.current_length_data.zero_() # this is for rerun\n",
    "    medusa_logits, outputs, logits = model(input_ids, output_orig = True, past_key_values=model.past_key_values)\n",
    "    inference_count += 1\n",
    "\n",
    "    medusa_pred = torch.argmax(medusa_logits[..., -1, :], dim = -1)\n",
    "    pred = torch.argmax(logits[..., -1, :], dim = -1)\n",
    "    preds = torch.cat([pred, medusa_pred[:, 0 ]], dim = -1)\n",
    "    print(f'Prediction @ {inference_count}: {tokenizer.batch_decode(pred)}')\n",
    "    cur_length = input_len\n",
    "    accept_lengths.append(1)\n",
    "    for _ in range(1024):\n",
    "        medusa_logits, outputs, logits = model(preds.cuda().unsqueeze(0), output_orig = True, past_key_values = model.past_key_values)\n",
    "        inference_count += 1\n",
    "\n",
    "        medusa_pred = torch.argmax(medusa_logits[..., -5:, :], dim = -1)\n",
    "        pred = torch.argmax(logits[..., :, :], dim = -1)\n",
    "        posterior_mask = (\n",
    "                    preds[1:] == pred[0, :-1]\n",
    "                ).int()\n",
    "        accept_length = torch.cumprod(posterior_mask, dim = -1).sum().item()\n",
    "        cur_length = cur_length + accept_length + 1\n",
    "        # update kv cache\n",
    "        model.current_length_data.fill_(cur_length)\n",
    "        # create new input\n",
    "        preds = torch.cat([pred[:, accept_length], medusa_pred[:,0,accept_length]], dim = -1)\n",
    "        print(f'Prediction @ {inference_count}: {tokenizer.batch_decode(pred[0, :accept_length + 1])}')\n",
    "        accept_lengths.append(accept_length + 1)\n",
    "        if tokenizer.eos_token_id in pred[0, :accept_length + 1]:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(accept_lengths)\n",
    "plt.xlabel('Inference step')\n",
    "plt.ylabel('Accept length')\n",
    "print('Avg. accept length:', np.mean(accept_lengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unleash the power of Medusa by Tree Attention\n",
    "\n",
    "Only a 1.58x speedup? You might think that's underwhelming. However, remember what we implemented in the previous section was merely the list structure of Medusa heads. This means we utilized only the Top-1 prediction from each Medusa head. In this section, we will introduce the Tree Attention mechanism, which is the key to unlocking the full potential of Medusa heads. We show a tree structure of Medusa heads below.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <picture>\n",
    "  <img src=\"../assets/medusa_choices.png\" width=\"100%\">\n",
    "  </picture>\n",
    "  <br>\n",
    "  <div align=\"left\" width=\"80%\">\n",
    "  <em>  We won't dive deep into how this tree was constructed. In simple terms, the tree shows us the optimized paths chosen by an algorithm that looks for the highest accuracy expectations under constraints. At every depth level, childrens from one node moving from left to right shows the top-k selections for each Medusa head. Noticeably, the Top-0 of Medusa head 1 has the most branching out, making the tree tilt more to the left. The left-heavy nature of the tree indicates that the top choices (shallower on the left) tend to have higher accuracy expectations. This suggests that the earliler (head 1 or 2) or topmost (top 1 or 2) choices are more reliable in their predictions. \n",
    "  </em>\n",
    "  </div>\n",
    "  <br>\n",
    "</div>\n",
    "\n",
    "Let's try the tree attention with practical example!\n",
    "\n",
    "### Initial inference with Medusa heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    new_token = 0\n",
    "    input_ids = tokenizer([prompt]).input_ids\n",
    "    input_len = len(input_ids[0])\n",
    "    input_ids = torch.as_tensor(input_ids).cuda()\n",
    "    model.current_length_data.zero_() # this is for rerun\n",
    "    reset_medusa_mode(model)\n",
    "    medusa_buffers = generate_medusa_buffers(\n",
    "                medusa_choices, device=model.base_model.device\n",
    "            )\n",
    "    medusa_logits, logits = initialize_medusa(\n",
    "            input_ids, model, medusa_buffers[\"medusa_attn_mask\"], past_key_values\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a function `generate_candidates` to retrieve the top-k predictions on the tree.\n",
    "\n",
    "`tree_candidates` contains the predictions on the nodes of the tree (also include the root node which is the model's head prediction).\n",
    "\n",
    "The `candidates` are all the paths (total 42) from the root node to the leaf nodes, padded with `<unk>` token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates, tree_candidates = generate_candidates(\n",
    "                medusa_logits,\n",
    "                logits,\n",
    "                medusa_buffers[\"tree_indices\"],\n",
    "                medusa_buffers[\"retrieve_indices\"],\n",
    "            )\n",
    "print('Candidates shape:', candidates.shape)\n",
    "print('Tree candidates shape:', tree_candidates.shape)\n",
    "print('Most left 2 candidates path:', tokenizer.batch_decode(candidates[0]), tokenizer.batch_decode(candidates[1]))\n",
    "print('Another candidate path:', tokenizer.batch_decode(candidates[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second inference with Medusa heads\n",
    "\n",
    "Like the previous section, we still need to verify the Medusa heads' prediction. Since we perform the greedy decoding, we set the `temperature` to **0.0** and ignore the `posterior_threshold` and `posterior_alpha` parameters. \n",
    "\n",
    "We have two functions, `tree_decoding` and `evaluate_posterior`.\n",
    "\n",
    "The `tree_decoding` performs the tree-attention-based inference.\n",
    "\n",
    "The `evaluate_posterior` performs the verification of the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    medusa_logits, logits, outputs = tree_decoding(\n",
    "                model,\n",
    "                tree_candidates,\n",
    "                past_key_values,\n",
    "                medusa_buffers[\"medusa_position_ids\"],\n",
    "                input_ids,\n",
    "                medusa_buffers[\"retrieve_indices\"],\n",
    "            )\n",
    "    best_candidate, accept_length = evaluate_posterior(\n",
    "                logits, candidates, temperature = 0, posterior_threshold = 0, posterior_alpha = 0\n",
    "            )\n",
    "    print('Medusa logits shape', medusa_logits.shape)\n",
    "    print('Logits shape', logits.shape)\n",
    "    print('Best candidate path index:', best_candidate.item())\n",
    "    print('Accept length:', accept_length.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the best candidate path looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Retrieved input @ best candidate:', tokenizer.batch_decode(candidates[best_candidate.item()]))\n",
    "print('Retrieved output @ best candidate:', tokenizer.batch_decode(logits.argmax(-1)[best_candidate.item()]))\n",
    "\n",
    "print('Retrieved input @ another candidate:', tokenizer.batch_decode(candidates[0]))\n",
    "print('Retrieved output @ another candidate:', tokenizer.batch_decode(logits.argmax(-1)[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The verification is very, very similar to what we discussed in the previous section, **shift and compare**. \n",
    "\n",
    "Best path looks like this:\n",
    "<table border=\"1\">\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>Input</th>\n",
    "            <td>'Once'</td>\n",
    "            <td>'upon'</td>\n",
    "            <td>'a'</td>\n",
    "            <td>'time'</td>\n",
    "            <td>'&lt;unk&gt'</td>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <th>Model output</th>\n",
    "            <td>'upon'<br>&#9989</td>\n",
    "            <td>'a'<br>&#9989</td>\n",
    "            <td>'time'<br>&#9989</td>\n",
    "            <td>','<br>&#x274C</td>\n",
    "            <td>'char'<br>&#x274C</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "Another path looks like this:\n",
    "<table border=\"1\">\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>Input</th>\n",
    "            <td>'Once'</td>\n",
    "            <td>'upon'</td>\n",
    "            <td>'ly'</td>\n",
    "            <td>'time'</td>\n",
    "            <td>'a'</td>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <th>Model output</th>\n",
    "            <td>'upon'<br>&#9989</td>\n",
    "            <td>'a'<br>&#9989</td>\n",
    "            <td>'a'<br>&#x274C</td>\n",
    "            <td>','<br>&#x274C</td>\n",
    "            <td>'char'<br>&#x274C</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "The best path contains more accepted tokens, so it win the competition!\n",
    "\n",
    "Now we will manage the KV cache (as we discussed before) and choose the correct input tokens on the path and it's medusa predictions for next tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, logits, medusa_logits, new_token = update_inference_inputs(\n",
    "                input_ids,\n",
    "                candidates,\n",
    "                best_candidate,\n",
    "                accept_length,\n",
    "                medusa_buffers[\"retrieve_indices\"],\n",
    "                outputs,\n",
    "                logits,\n",
    "                medusa_logits,\n",
    "                new_token,\n",
    "                past_key_values_data,\n",
    "                current_length_data,\n",
    "            )\n",
    "print('Decode:', tokenizer.batch_decode(input_ids[:,input_len:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It's time to put all the things together!\n",
    "\n",
    "Below is the whole process of the tree attention inference. The acceleration ratio is boosted from ~1.6X to ~2.4X!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accept_lengths_tree = []\n",
    "\n",
    "with torch.inference_mode():\n",
    "    new_token = 0\n",
    "    input_ids = tokenizer([prompt]).input_ids\n",
    "    input_len = len(input_ids[0])\n",
    "    input_ids = torch.as_tensor(input_ids).cuda()\n",
    "    model.current_length_data.zero_() # this is for rerun\n",
    "    reset_medusa_mode(model)\n",
    "    medusa_buffers = generate_medusa_buffers(\n",
    "                medusa_choices, device=model.base_model.device\n",
    "            )\n",
    "    medusa_logits, logits = initialize_medusa(\n",
    "            input_ids, model, medusa_buffers[\"medusa_attn_mask\"], past_key_values\n",
    "        )\n",
    "    cur_length = input_len + 1\n",
    "    accept_lengths_tree.append(1)\n",
    "    \n",
    "    for i in range(1024):\n",
    "        candidates, tree_candidates = generate_candidates(\n",
    "                medusa_logits,\n",
    "                logits,\n",
    "                medusa_buffers[\"tree_indices\"],\n",
    "                medusa_buffers[\"retrieve_indices\"],\n",
    "            )\n",
    "        medusa_logits, logits, outputs = tree_decoding(\n",
    "                model,\n",
    "                tree_candidates,\n",
    "                past_key_values,\n",
    "                medusa_buffers[\"medusa_position_ids\"],\n",
    "                input_ids,\n",
    "                medusa_buffers[\"retrieve_indices\"],\n",
    "            )\n",
    "        best_candidate, accept_length = evaluate_posterior(\n",
    "                logits, candidates, temperature = 0, posterior_threshold = 0, posterior_alpha = 0\n",
    "            )\n",
    "        input_ids, logits, medusa_logits, new_token = update_inference_inputs(\n",
    "                input_ids,\n",
    "                candidates,\n",
    "                best_candidate,\n",
    "                accept_length,\n",
    "                medusa_buffers[\"retrieve_indices\"],\n",
    "                outputs,\n",
    "                logits,\n",
    "                medusa_logits,\n",
    "                new_token,\n",
    "                past_key_values_data,\n",
    "                current_length_data,\n",
    "            )\n",
    "        \n",
    "        accept_length_tree = input_ids.shape[1] - cur_length\n",
    "        cur_length = accept_length_tree + cur_length\n",
    "        accept_lengths_tree.append(accept_length_tree)\n",
    "        if model.tokenizer.eos_token_id in input_ids[0, input_len:]:\n",
    "            break\n",
    "print('Decode:', tokenizer.batch_decode(input_ids[:,input_len:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(data, window_size=5):\n",
    "    return np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "# Unsmoothed plots with transparency\n",
    "plt.plot(accept_lengths_tree, label='Tree decoding (Raw)', alpha=0.3)\n",
    "plt.plot(accept_lengths, label='List decoding (Raw)', alpha=0.3)\n",
    "\n",
    "# Smoothed plots\n",
    "window_size = 5  # You can adjust this as needed\n",
    "plt.plot(moving_average(accept_lengths_tree, window_size), label='Tree decoding (Smoothed)', color='tab:blue')\n",
    "plt.plot(moving_average(accept_lengths, window_size), label='List decoding (Smoothed)', color='tab:orange')\n",
    "\n",
    "plt.xlabel('Inference step')\n",
    "plt.ylabel('Accept length')\n",
    "plt.legend()\n",
    "\n",
    "print('Avg. accept tree length:', np.mean(accept_lengths_tree))\n",
    "print('Avg. accept list length:', np.mean(accept_lengths))\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
